ГЛАВА 1<br>
<br>
Для чего нужно<br>
обучение с подкреплением?<br>
Как люди учатся? Этот обманчиво простой вопрос сбивал с толку мыслителей на<br>
протяжении тысячелетий. Греческий философ Платон и его ученик Аристотель за­<br>
дались вопросом: находятся ли истина и знание внутри нас (рационализм) или они<br>
пережиты (эмпиризм)? Даже сегодня, 2500 лет спустя, люди все еще пытаются от­<br>
ветить на этот вечный вопрос.<br>
Если бы люди уже всё знали, им не нужно было бы больше приобретать жизнен­<br>
ный опыт. Люди могли бы проводить остаток своего земного времени, улучшая<br>
жизнь, принимая правильные решения и размышляя над такими важными вопроса­<br>
ми, как &quot;где мои ключи?” и ”я запер входную дверь?”. Но как люди вообще полу­<br>
чают эти знания? Вы можете научить знанию. А более высокий уровень среднего<br>
образования ведет к лучшему обществу. Но всему нельзя научить. И на уроках,<br>
и в жизни ученик должен переживать.<br>
<br>
Маленькие дети вдохновляют в этом отношении. Им нужно испробовать ряд си­<br>
туаций и результатов. В долгосрочной перспективе они начинают искать полезный<br>
опыт и избегать пагубного (хочется надеяться). Они активно принимают решения и<br>
оценивают результаты. Но жизнь ребенка загадочна, и награды часто вводят в за­<br>
блуждение. Немедленная награда за то, что ребенок залезет в шкаф и съест печенье,<br>
велика, но наказание будет суровее.<br>
<br>
Обучение с подкреплением объединяет две задачи. Первая — это исследование но­<br>
вых ситуаций. Вторая — использование этого опыта для принятия более качест­<br>
венных решений. Со временем так формируется план достижения цели. Например,<br>
ребенок учится ходить, вставая, наклоняясь вперед и падая в объятия любящего<br>
родителя. Но это только после многих часов хождения, держась за руки, шатания<br>
и падений. В конце концов, мышцы ног ребенка начинают работать слаженно,<br>
используя многоступенчатую стратегию, которая сообщает, что и когда нужно де­<br>
лать. Вы не можете вложить в голову ребенку решение всех жизненных ситуаций,<br>
которое когда-либо ему понадобится, поэтому вместо этого жизнь предоставляет<br>
ребенку основу, на которой можно учиться.<br>
В этой книге показано, как реализовать процесс подкрепления для компьютера. Но<br>
зачем это делать? Это позволяет машине учиться самостоятельно. Перефразируя<br>
любимое телешоу моей жены, скажу: вы даете машине возможность искать новые<br>
впечатления, смело идти туда, куда раньше не ступала никакая машина.<br>
В этой главе представлено введение в обучение с подкреплением (я откладываю<br>
формальное определение обучения с подкреплением до главы 2). Во-первых,<br>
<br>
30<br>
<br>
|<br>
<br>
Гпава 1<br>
<br>
я опишу, зачем это искусство нужно инженерам и почему именно сейчас. К концу<br>
главы вы узнаете, в каких отраслях можно использовать обучение с подкреплени­<br>
ем, и разработаете свою первую математическую модель. Также будет дан обзор<br>
тех типов алгоритмов, с которыми вы встретитесь позже в этой книге.<br>
<br>
В этой книге я употребляю слово &quot;инженер”, чтобы абстрактно говорить обо<br>
всех, кто использует свои навыки для разработки решения проблемы. Я имею<br>
в виду инженеров-программистов, инженеров по обработке данных, специали­<br>
стов по данным, исследователей и т. д.<br>
<br>
Почему сейчас?<br>
Две причины обусловили необходимость и способность выполнять обучение с под­<br>
креплением: доступ к большим объемам данных и возросшая скорость обработки<br>
данных.<br>
<br>
Вплоть до 2000 г. человеческие знания хранились на аналоговых устройствах, та­<br>
ких как книги, газеты и магнитные ленты. Если бы вы сжали эти знания, то<br>
в 1993 г. вам потребовалось бы 15,8 эксабайт пространства (один эксабайт равен<br>
одному миллиарду гигабайт) [1]. В 2018 г. этот показатель увеличился до<br>
33 зеттабайт (1 Збайт = 1 000 000 000 000 Гбайт). Поставщикам облачных услуг да­<br>
же приходится прибегать к использованию жестких дисков размером с контейнер<br>
для загрузки больших объемов данных [2].<br>
Вам также потребуются необходимые вычислительные мощности для анализа всех<br>
этих данных. В качестве демонстрации давайте рассмотрим случай одной из самых<br>
ранних реализаций обучения с подкреплением.<br>
В 1947 г. Дитрих Принц (Dietrich Prinz) работал на компанию Ferranti в Манчестере<br>
(Великобритания). Там он помог спроектировать и сконструировать первую произ­<br>
водственную версию манчестерского компьютера под названием Ferranti Mark 1 [3].<br>
Он научился программировать Mark 1 под руководством Алана Тьюринга (Alan<br>
Turing) и Сисели Попплуэлл (Cicely Popplewell). Под влиянием статьи Тьюринга на<br>
эту тему в 1952 г. Принц выпустил шахматную программу, которая могла решать<br>
единственный набор задач под названием &quot;matein-2”. Это шахматные композиции,<br>
в которых игрок выбирает два хода, приводящих к мату в шахматах. Алгоритм<br>
Принца тщательно перебирал все возможные позиции и вырабатывал решение<br>
в среднем за 15-20 минут. Это реализация алгоритма Монте-Карло, описанного<br>
в главе 2. Принц стал рассматривать шахматное программирование как ’’ключ к ме­<br>
тодам, которые можно использовать для решения структурных или логистических<br>
задач в других областях с помощью электронных компьютеров”. Он был прав [4].<br>
Одновременное наращивание объема данных и вычислительной мощности аппа­<br>
ратного обеспечения привело к тому, что примерно в 2010 г. стало возможным и<br>
необходимым учить машины обучаться.<br>
<br>
Для чего нужно обучение с подкреплением?<br>
<br>
|<br>
<br>
31<br>
<br>
Машинное обучение<br>
Полное описание машинного обучения выходит за рамки этой книги. Но на ма­<br>
шинном обучении основано обучение с подкреплением. Прочтите как можно<br>
больше о машинном обучении, особенно о книгах, которые я рекомендую<br>
в разд. &quot;Дополнительные материалы для чтения &quot; в конце этой главы.<br>
Повсеместное распространение данных и доступность дешевых высокопроизводи ­<br>
тельных вычислений позволили исследователям пересмотреть алгоритмы 1950-х го­<br>
дов. Они выбрали название &quot;машинное обучение&quot; (machine learning, ML), но такое<br>
название не вполне удачно, потому что ML одновременно считается и дисципли­<br>
ной, и набором методов. Я считаю машинное обучение детищем науки о данных<br>
(data science), которая представляет собой всеобъемлющую научную область, изу­<br>
чающую данные, генерируемые явлениями. Мне не нравится термин &quot;искусствен­<br>
ный интеллект&quot; (ИИ— artificial intelligence, AI) по той же причине; достаточно<br>
сложно определить, что такое интеллект, не говоря уже о том, как он воплощается.<br>
<br>
ML начинается с большого количества информации в виде данных, полученных<br>
в ходе наблюдений. Наблюдение представляет собой набор атрибутов в единой точ­<br>
ке, которые описывают сущность. Например, в избирательном опросе одно наблю­<br>
дение представляет собой предполагаемый голос одного человека. Для задачи фор­<br>
мулирования рекомендаций наблюдением может быть щелчок по определенному<br>
продукту. Инженеры используют ML-алгоритмы для интерпретации этой инфор­<br>
мации и принятия решений.<br>
<br>
При обучении с учителем метки представляют ответ на проблему для конкретного<br>
наблюдения. Здесь алгоритм пытается использовать информацию, чтобы угадать<br>
правильный результат. Обучение без учителя работает без меток, и вы принимаете<br>
решения на основе характеристик данных. Я всегда рекомендую своим клиентам из<br>
Winder Research стремиться к контролируемому обучению — например, путем оп­<br>
латы или проведения экспериментов для поиска меток, — потому что, если у вас<br>
нет основополагающей истины, вам будет сложно количественно оценить эффек­<br>
тивность.<br>
<br>
Процесс поиска алгоритма решения задачи называется моделированием. Инженеры<br>
проектируют модели для упрощения и представления основных явлений. Они ис­<br>
пользуют модель, чтобы делать обоснованные предположения о новых наблюдени­<br>
ях. Например, модель может сказать вам, что новый клиент предоставил ложную<br>
информацию в своем приложении, или может преобразовать вашу речь в текст.<br>
Учитывая эти описания, попробуйте научить ребенка кататься на велосипеде. Как<br>
лучше всего это сделать? Согласно парадигме ML вы должны разметить множество<br>
наблюдений. Вы можете посоветовать своему ребенку посмотреть видео с профес­<br>
сиональными велосипедистами. Как только он просмотрит достаточное количество<br>
видеороликов, вы, игнорируя любые его протесты о том, что ему было скучно,<br>
можете проверить его способности в соответствии с некоторыми произвольными<br>
техническими критериями успеха. Думаете, это сработает? Нет.<br>
<br>
32<br>
<br>
|<br>
<br>
Гпава 1<br>
<br>
Несмотря на то что ML принципиально подходит для многих прикладных задач,<br>
некоторые проблемы не поддаются машинному обучению. Лучшее решение, про­<br>
должая предыдущий пример, — позволить своему ребенку попробовать самостоя­<br>
тельно прокатиться. Некоторые его попытки ничем не увенчаются. В других случа­<br>
ях у него что-то получится. Каждое решение будет сказываться на его представле­<br>
нии о задаче. После достаточного количества попыток и определенных наставлений<br>
он изучит стратегии, позволяющие максимизировать собственное определение<br>
успеха. Вот в чем обучение с подкреплением превосходит обучение с учителем.<br>
<br>
Обучение с подкреплением<br>
Обучение с подкреплением (reinforcement learning, RL) поясняет, как принимать<br>
наилучшие решения последовательно, в определенном контексте, чтобы максими­<br>
зировать реальный показатель успеха. Лицо, принимающее решения, узнает об<br>
этом методом проб и ошибок. Ему не говорят, какие именно решения принимать,<br>
вместо этого он должен учиться самостоятельно, методом проб и ошибок. На<br>
рис. 1.1 представлены четыре компонента RL, в главе 2 мы углубимся в подроб­<br>
ности.<br>
<br>
а<br>
<br>
б<br>
<br>
Рис. 1.1. Набросок четырех компонентов, необходимых для RL: агента, который совершает<br>
действия в окружающей среде для наибольшего вознаграждения. Пример (а)<br>
демонстрирует робота, который намеревается пройти через лабиринт, чтобы получить<br>
монету. Пример (б) показывает приложение для электронной коммерции,<br>
которое автоматически добавляет товары в корзины пользователей,<br>
чтобы максимизировать прибыль<br>
<br>
Каждое решение — это действие. Например, когда вы едете на велосипеде, дейст­<br>
виями являются рулевое управление, кручение педалей и торможение. Если вы<br>
пытаетесь автоматически добавлять товары в корзину, то такими действиями явля­<br>
ются решения о добавлении определенных товаров.<br>
<br>
Для чего нужно обучение с подкреплением?<br>
<br>
|<br>
<br>
33<br>
<br>
Контекст, хотя он может отражать любую реальную ситуацию, часто ограничен,<br>
что не позволяет сделать проблему разрешимой. Практики RL позволяют подгото­<br>
вить своеобразный интерфейс взаимодействия с окружающей средой. Это может<br>
быть симуляция, реальная жизнь или их комбинация. Окружающая среда принима­<br>
ет действия и отвечает на них результатом и новым набором наблюдений.<br>
Агент — это субъект, который принимает решения. Это может быть ваш ребенок,<br>
какая-нибудь программа или, например, робот.<br>
<br>
Вознаграждение кодирует вызов. Этот механизм обратной связи сообщает агенту,<br>
какие действия привели к успеху (или неудаче).<br>
Сигнал вознаграждения обычно числовой, но нужен только для подкрепления по­<br>
ведения; например, стратегии генетического обучения могут удалять неэффектив­<br>
ных агентов и не предоставлять никакого вознаграждения.<br>
Вот еще пример: вы можете вознаградить робота за достижение цели или агента за<br>
добавление нужного продукта в корзину. Все просто, правда? Но что делать, если<br>
роботу требуется три дня, чтобы выйти из простого лабиринта, потому что он про­<br>
водит большую часть времени, нарезая круги? А если агент начнет добавлять все<br>
подряд товары в корзину?<br>
Такие процессы происходят и в мире животных. Они должны максимально увели­<br>
чить свои шансы на выживание, чтобы передать свои гены потомству. Например,<br>
как и большинству травоядных, лосям нужно много есть, чтобы выжить. Но<br>
в 2011 г. в окрестностях Гётеборга (Швеция) нашли лося, застрявшего в ветвях де­<br>
рева после того, как он наелся ферментированных яблок [5]. Система ’’вознаграж­<br>
дения” лося, которая вызывает голод, дала сбой, потому что цель ее слишком ли­<br>
шена конкретики. Нельзя есть все подряд, чтобы максимизировать свои шансы на<br>
выживание. Все гораздо сложнее.<br>
<br>
Эти примеры подводят нас к главной проблеме в RL, которая известна с тех пор,<br>
как Ада Лавлейс (Ada Lovelace) впервые написала алгоритм для получения чисел<br>
Бернулли. Как сказать машине, что она должна делать? Агенты RL часто остаются<br>
крайними, потому что они оптимизируются не для того, что на самом деле нужно.<br>
И пока я рекомендую вам максимально не усложнять награду. Многие задачи<br>
предполагают естественную награду. В главе 9 эта проблема обсуждается более<br>
подробно.<br>
Итак, четыре компонента образуют марковский процесс принятия решений (Markov<br>
decision process, MDP). MDP используют для того, чтобы сформулировать задачи,<br>
даже не связанные с инженерией. В главе 2 эти идеи представлены более подробно.<br>
<br>
Когда следует использовать<br>
обучение с подкреплением?<br>
Некоторые примеры RL, которые вы найдете в Интернете, выглядят вымученными.<br>
Их авторы берут пример ML и пытаются применить к нему RL, несмотря на отсут­<br>
ствие четкого агента или действия. Посмотрите, например, несколько примеров<br>
с попытками включить RL в прогнозирование фондового рынка. Существует воз-<br>
<br>
34<br>
<br>
|<br>
<br>
Гпава 1<br>
<br>
можность использования автоматизированного агента для совершения сделок, но<br>
во многих примерах это не главное; основное внимание по-прежнему уделяется<br>
прогнозной модели. Это неуместно, и такие примеры лучше оставить для ML.<br>
RL работает лучше всего, когда решения принимаются последовательно, а действия<br>
связаны с исследованием окружающей среды. Возьмите робототехнику, это клас­<br>
сическая область применения RL. Цель робота — научиться выполнять неизвест­<br>
ные задачи. Вы не должны указывать роботу, как добиться успеха, потому что это<br>
либо слишком сложно (допустим, вы просите робота построить дом), либо вы мо­<br>
жете быть предвзяты в силу собственного опыта (вы не робот), поэтому вы не знае­<br>
те, как поставить себя на место робота. Если вместо этого вы позволите роботу<br>
провести исследование, он сможет найти оптимальное решение. Этот случай хоро­<br>
шо подходит для RL.<br>
<br>
Всегда выбирайте самое простое решение, которое удовлетворительно решает<br>
вашу прямую задачу.<br>
<br>
Основное преимущество RL заключается в том, что такое обучение оптимизирует­<br>
ся для получения долгосрочных многоэтапных вознаграждений. Второстепенное<br>
преимущество состоит в том, что очень легко включить в процесс метрики, исполь­<br>
зуемые бизнесом. Например, рекламные решения обычно оптимизированы в целях<br>
обеспечения наилучшей кликабельности для отдельной рекламы. Это неоптималь­<br>
но, потому что зрители часто видят несколько рекламных объявлений, а цель не<br>
щелчок (клик), а нечто большее, например удержание потребителя, регистрация<br>
или покупка. Комбинация показываемых рекламных объявлений (в определенном<br>
порядке и с конкретным содержанием) может быть автоматически оптимизирова­<br>
на RL с помощью простой в использовании цели, соответствующей потребностям<br>
бизнеса.<br>
Вы можете отказаться от некоторых из четырех компонентов, представленных<br>
в предыдущем разделе, чтобы упростить разработку. Если в вашей модели нет ес­<br>
тественного сигнала, свидетельствующего о вознаграждении, например, ’’робот<br>
достиг цели”, то можно создать искусственное вознаграждение. Также часто соз­<br>
дают симуляцию окружающей среды. Вы можете квантовать или обрывать дейст­<br>
вия. Но это все компромиссы. Симуляция никогда не заменит реальный жизненный<br>
опыт.<br>
В RL активно ищется оптимальная модель. Вам не нужно создавать случайную вы­<br>
борку и подстраивать ее в автономном режиме. Быстрое онлайн-обучение может<br>
творить чудеса, когда важно в самые сжатые сроки добиться максимальной произ­<br>
водительности. Например, в А/В-тестах, ориентированных на прибыль, когда нуж­<br>
но решить, какой маркетинговый текст использовать, не хочется тратить время на<br>
подготовку случайной выборки, если какой-то вариант недостаточно эффективен.<br>
RL делает это бесплатно. О том, как А/В-тестирование соотносится с RL, вы може­<br>
те узнать в главе 2.<br>
<br>
Для чего нужно обучение с подкреплением?<br>
<br>
|<br>
<br>
35<br>
<br>
Таким образом, RL лучше всего подходит для прикладных задач, которые тре­<br>
буют последовательных, сложных решений и имеют долгосрочную цель (в кон­<br>
тексте единственного решения). ML может помочь вам в качестве вспомога­<br>
тельного инструмента, но RL лучше всего подходит для сред с прямой обратной<br>
связью. Утверждаю это, поскольку я разговаривал с некоторыми практиками,<br>
которые использовали RL для замены групп специалистов по обработке дан­<br>
ных, настраивающих производительность решений машинного обучения.<br>
<br>
Варианты применения обучения с подкреплением<br>
В этой книге я привожу целый спектр примеров по двум причинам. Во-первых,<br>
я хочу проиллюстрировать теоретические аспекты, например, как работают алго­<br>
ритмы. Эти примеры просты и абстрактны. Лично я считаю, что просмотр приме­<br>
ров помогает мне учиться. Я также рекомендую вам воспроизвести примеры, это<br>
поможет вам в обучении. Во-вторых, я хочу показать, как использовать RL в про­<br>
мышленности.<br>
В СМИ, как правило, наибольшее внимание уделяется примерам, демонстрирую­<br>
щим, как агенты побеждают людей в играх. Журналистам нравятся броские исто­<br>
рии о том, как люди сдают свои позиции. А ученые продолжают обращаться к иг­<br>
рам из-за сложной моделируемой среды. Но я решил не говорить ни о DeepMind<br>
AlphaGo Zero, ни о версии агента, победившего чемпиона мира по го, ни об OpenAI<br>
Five, победившей чемпионов мира по Dota 2, а вместо этого сосредоточиться на<br>
приложениях и примерах из самых разных промышленных отраслей. Я не говорю,<br>
что игровые примеры — пустая трата времени. Игровые компании могут использо­<br>
вать RL для многих практических целей, например для помощи в тестировании или<br>
оптимизации внутриигровых вариантов &quot;АГ для максимизации дохода. Мне хочет­<br>
ся помочь вам абстрагироваться от хайпа и показать разнообразные области, где<br>
применимо RL. Для того чтобы продемонстрировать, что именно возможно уже<br>
сейчас, я представляю широкий выбор экспериментов, которые лично мне кажутся<br>
интересными.<br>
<br>
♦ Область робототехники имеет множество приложений RL, включая улучшение<br>
движения и производственного процесса, игру в бильбоке и переворачивание<br>
блинов [6]. Автономные транспортные средства также являются темой активных<br>
исследований [7].<br>
♦ Вы можете использовать RL для улучшения облачных вычислений. В одной ста­<br>
тье рассказано, как оптимизируются приложения с учетом задержки [8], в дру­<br>
гой обсуждается соотношение &quot;энергоэффективность/использование&quot; [9]. Охла­<br>
ждение центра обработки данных, охлаждение процессора и сетевая маршрути­<br>
зация — все это варианты применения RL, используемые сегодня [10-12].<br>
♦ Финансовая отрасль применяет RL для совершения сделок и распределения<br>
портфеля [13, 14]. Также существует значительный интерес к оптимизации<br>
ценообразования в режиме реального времени [15].<br>
♦ Количество энергии, потребляемой при коммунальном обслуживании (через<br>
отопление, воду, свет и т. д.), может быть значительно уменьшено с помощью<br>
<br>
36<br>
<br>
|<br>
<br>
Гпава 1<br>
<br>
RL [16]. А электрические сети могут использовать RL для решения ситуаций,<br>
когда спрос неоднороден; дома являются одновременно производителями и по­<br>
требителями [17].<br>
<br>
♦ RL улучшает управление светофорами и активное управление полосами движе­<br>
ния [18, 19]. Умные города также остаются в выигрыше [20].<br>
<br>
♦ Недавние статьи предлагают множество вариантов применения RL в здраво­<br>
охранении, особенно в областях дозирования и составления схем лечения [21,<br>
22]. RL можно использовать для разработки более совершенных протезов и про­<br>
тезных контроллеров [23].<br>
♦ Система образования и электронное обучение могут выиграть благодаря при­<br>
цельно подобранным учебным программам на основе RL [24].<br>
Ни один бизнес-сектор не остался незатронутым: игры, технологии, транспорт, фи­<br>
нансы, наука и окружающая среда, промышленность, производство и государст­<br>
венные службы — все они ссылались на приложения RL.<br>
<br>
Я не хочу терять вас в бесконечном списке, поэтому вместо этого я отсылаю<br>
вас на соответствующий веб-сайт1, где у меня есть полный каталог приложе­<br>
ний RL.<br>
<br>
Любая технология опасна в шаловливых руках. И, помня о популистских аргумен­<br>
тах против AI, можно интерпретировать RL как опасное явление. Прошу вас, как<br>
инженер, как человек, подумать о том, что вы строите. Прикиньте, как это повлияет<br>
на других людей? Какие есть риски? Это противоречит вашей морали? Будьте от­<br>
ветственны за свою работу перед собой. Если вы не можете этого сделать, вам,<br>
вероятно, не следует этим заниматься. Далее приведены еще три задокументиро­<br>
ванных гнусных приложения. У каждого свои этические границы,. Где ваша грани­<br>
ца? Какие приложения вам подходят?<br>
♦ Pwnagotchi— это устройство на базе RL, которое активно сканирует, анали­<br>
зирует и взламывает Wi-Fi-сети с WPA/WPA2-3anjHToft путем дешифрования<br>
рукопожатий [25].<br>
<br>
♦ Исследователи показали, что можно обучить агентов обходить статические<br>
модели вредоносных программ в антивирусных сканерах [26].<br>
♦ Военное ведомство США разрабатывает модели боевых действий, чтобы проде­<br>
монстрировать, как автономные роботы могут помочь на поле боя [27].<br>
Я более подробно обсуждаю вопросы безопасности и этики в главе 10,<br>